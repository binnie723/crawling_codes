import os
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import requests
import pandas as pd
from bs4 import BeautifulSoup
import base64
import re
from webdriver_manager.chrome import ChromeDriverManager
from concurrent.futures import ThreadPoolExecutor, as_completed

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CATEGORY_DIR = os.path.join(BASE_DIR, "watch")
CAT_OUTPUT_DIR = os.path.join(CATEGORY_DIR, "watch_output")
THUMBNAIL_DIR = os.path.join(CATEGORY_DIR, "ì¸ë„¤ì¼")
DETAIL_IMAGES_DIR = os.path.join(CATEGORY_DIR, "ìƒì„¸ì‚¬ì§„")

if not os.path.exists(CAT_OUTPUT_DIR):
    os.makedirs(CAT_OUTPUT_DIR)
if not os.path.exists(THUMBNAIL_DIR):
    os.makedirs(THUMBNAIL_DIR)
if not os.path.exists(DETAIL_IMAGES_DIR):
    os.makedirs(DETAIL_IMAGES_DIR)
    
print("--- ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ ---")

def setup_driver():
    chrome_options = ChromeOptions()
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    user_agent_string = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    chrome_options.add_argument(f"user-agent={user_agent_string}")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-logging"])
    chrome_options.add_argument("--incognito")
    chrome_options.page_load_strategy = 'normal'
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.set_window_size(1200, 800)
    print("--- WebDriver ì„¤ì • ì™„ë£Œ ---")
    return driver

def save_image(image_url, filename, folder_path):
    if not image_url.startswith("data:image") and image_url.startswith("//"):
        image_url = "https:" + image_url
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        file_path = os.path.join(folder_path, filename)
        if image_url.startswith("data:image"):
            match = re.match(r"data:image/(png|jpeg|jpg);base64,(.*)", image_url)
            if match:
                base64_data = match.group(2)
                image_bytes = base64.b64decode(base64_data)
                with open(file_path, 'wb') as out_file:
                    out_file.write(image_bytes)
                print(f"  > BASE64 ì´ë¯¸ì§€ ì €ì¥: {filename}")
            else:
                print(f"  > BASE64 í˜•ì‹ ë¶ˆì¼ì¹˜: {image_url[:50]}...")
                pass
        else:
            response = requests.get(image_url, stream=True, headers=headers)
            response.raise_for_status()
            with open(file_path, 'wb') as out_file:
                out_file.write(response.content)
            print(f"  > URL ì´ë¯¸ì§€ ì €ì¥: {filename}")

        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                print(f"  > íŒŒì¼ í¬ê¸°ê°€ 0ë°”ì´íŠ¸ì…ë‹ˆë‹¤: {filename}")
                pass
            else:
                pass
        else:
            print(f"  > íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {filename}")
            pass

    except requests.exceptions.RequestException as e:
        print(f"  > ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (RequestsError): {filename}, URL: {image_url}, ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"  > ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì¼ë°˜ ì˜¤ë¥˜): {filename}, URL: {image_url}, ì˜¤ë¥˜: {e}")


def main(): 
    total_start_time = time.time()
    driver = setup_driver()
    all_product_data = []
    
    save_interval = 5
    last_save_count = 0
    current_page_number = 1
    thumbnail_counter = (current_page_number - 1)*20 + 1
    base_url = "https://thedaall-dn.com/category/watch/23/"
    
    try:
        print(f"\n--- ì›¹ì‚¬ì´íŠ¸ ì ‘ì†: {base_url} ---")
        driver.get(base_url)
        crawled_product_ids = set()
        
        while True:
            page_start_time = time.time()
            print(f"\n--- {current_page_number} í˜ì´ì§€ ìƒí’ˆ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì‹œì‘ ---")
            try:
                WebDriverWait(driver, 30).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'ul.prdList li'))
                )
            except Exception:
                print("ìƒí’ˆ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë˜í•‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            products_on_page = driver.find_elements(By.CSS_SELECTOR, "ul.prdList.grid4 li")
            
            newly_found_products = []
            for product in products_on_page:
                try:
                    product_id = product.get_attribute('id')
                    if product_id and product_id not in crawled_product_ids:
                        newly_found_products.append(product)
                        crawled_product_ids.add(product_id)
                except Exception:
                    continue
            
            if not newly_found_products:
                print(f"{current_page_number} í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë˜í•‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            product_info_list = []
            for i, product in enumerate(newly_found_products):
                absolute_index = len(crawled_product_ids) - len(newly_found_products) + i
                try:
                    thumbnail_element = product.find_element(By.CSS_SELECTOR, 'div.thumbnail a img')
                    thumbnail_url = thumbnail_element.get_attribute('src')
                    
                    product_name_element = product.find_element(By.CSS_SELECTOR, 'strong.name')
                    product_name = product_name_element.text.strip()
                    
                    product_link_element = product.find_element(By.CSS_SELECTOR, 'div.thumbnail a')
                    product_url = product_link_element.get_attribute('href')
                    
                    product_info_list.append({
                        'index': absolute_index,
                        'name': product_name,
                        'thumbnail_url': thumbnail_url,
                        'product_url': product_url
                    })
                except Exception as e:
                    print(f"ìƒí’ˆ ëª©ë¡ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            for product_info in product_info_list:
                i = product_info['index']
                product_name_on_list = product_info['name']
                thumbnail_url = product_info['thumbnail_url']
                product_url = product_info['product_url']
                
                product_start_time = time.time()
                current_product_data_raw = {}
                
                try:
                    print(f"\n--- [{thumbnail_counter}ë²ˆ ìƒí’ˆ] ìƒí’ˆëª…: {product_name_on_list} ---")
                    current_product_data_raw['ìˆœìœ„'] = thumbnail_counter
                    current_product_data_raw['í˜ì´ì§€'] = current_page_number
                    current_product_data_raw['ìƒí’ˆëª…'] = product_name_on_list
                    thumbnail_filename = f"{thumbnail_counter}.jpg"
                    print(f"  > ì¸ë„¤ì¼ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
                    save_image(thumbnail_url, thumbnail_filename, folder_path=THUMBNAIL_DIR)
                    current_product_data_raw['ì¸ë„¤ì¼ ì´ë¯¸ì§€ íŒŒì¼ëª…'] = thumbnail_filename
                    
                    print(f"  > ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ì ‘ì†: {product_url}")
                    product_page_start = time.time()
                    driver.get(product_url)
                    time.sleep(0.1)
                    
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    
                    # ğŸ’¡ ìƒí’ˆ ê°€ê²© ì •ë³´ ì¶”ì¶œ (ìƒˆë¡œìš´ ID ì‚¬ìš©)
                    price_element = soup.find('strong', id='span_product_price_text')
                    product_price = 'ê°€ê²© ì •ë³´ ì—†ìŒ'
                    if price_element:
                        product_price = price_element.text.strip()
                    
                    current_product_data_raw['ìƒí’ˆê°€ê²©'] = product_price
                    print(f"  > ìƒí’ˆ ê°€ê²© ì¶”ì¶œ ì™„ë£Œ: {product_price}")

                    prd_detail_div = soup.find('div', id='prdDetail')
                    if prd_detail_div:
                        detail_text = prd_detail_div.get_text(separator="\n", strip=True)
                        current_product_data_raw['ìƒí’ˆ ìƒì„¸'] = detail_text
                        print("  > ìƒí’ˆ ìƒì„¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
                    else:
                        current_product_data_raw['ìƒí’ˆ ìƒì„¸'] = None
                        print("  > ìƒí’ˆ ìƒì„¸ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                    all_detail_images_tags = soup.find_all('img')
                    detail_image_urls = []
                    for img_tag in all_detail_images_tags:
                        img_to_process = img_tag.get('ec-data-src') or img_tag.get('src')
                        if img_to_process:
                            if img_to_process.startswith("data:image"):
                                continue
                            if img_to_process.startswith("//"):
                                img_to_process = "https:" + img_to_process
                            elif not img_to_process.startswith('http'):
                                img_to_process = base_url.rstrip('/') + img_to_process
                            
                            if (('/detail/' in img_to_process or '/product/' in img_to_process) and 
                                img_to_process.lower().endswith(('.jpg', '.jpeg', '.png'))):
                                detail_image_urls.append(img_to_process)

                    detail_image_urls = list(dict.fromkeys(detail_image_urls))
                    
                    if detail_image_urls:
                        print(f"  > ìƒì„¸ ì´ë¯¸ì§€ {len(detail_image_urls)}ê°œ ë‹¤ìš´ë¡œë“œ ì‹œì‘ (ë©€í‹°ìŠ¤ë ˆë”©)")
                        def download_single_image(url_and_index):
                            url, index = url_and_index
                            detail_image_filename = f"{thumbnail_counter}_{index}.jpg"
                            time.sleep(0.5)
                            save_image(url, detail_image_filename, folder_path=DETAIL_IMAGES_DIR)
                            return detail_image_filename
                        
                        url_index_pairs = [(url, idx + 1) for idx, url in enumerate(detail_image_urls)]
                        
                        with ThreadPoolExecutor(max_workers=5) as executor:
                            futures = [executor.submit(download_single_image, pair) for pair in url_index_pairs]
                            downloaded_files = [future.result() for future in as_completed(futures)]
                        print("  > ìƒì„¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                    else:
                        print("  > ìƒì„¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                    all_product_data.append(current_product_data_raw)
                    
                    current_count = len(all_product_data)
                    should_save = (current_count - last_save_count >= save_interval or i == len(product_info_list) - 1)
                    
                    if should_save:
                        print(f"  > [{current_count}ê°œ ìƒí’ˆ] raw_data.csv íŒŒì¼ì— ì¤‘ê°„ ì €ì¥...")
                        df = pd.DataFrame(all_product_data)
                        csv_path = os.path.join(CAT_OUTPUT_DIR, "raw_data.csv")
                        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
                        last_save_count = current_count

                    driver.back()
                    print(f"  > ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ ëª©ë¡ í˜ì´ì§€ë¡œ ë³µê·€. ì†Œìš” ì‹œê°„: {time.time() - product_page_start:.2f}ì´ˆ")
                except Exception as e:
                    print(f"  > [{thumbnail_counter}ë²ˆ ìƒí’ˆ] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    try:
                        driver.back()
                    except:
                        print("  > ëª©ë¡ í˜ì´ì§€ë¡œ ëŒì•„ê°€ê¸° ì‹¤íŒ¨. ë‹¤ìŒ ìƒí’ˆìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                        pass
                    continue
                
                thumbnail_counter += 1
            
            page_navigation_start = time.time()
            more_button_xpath = '//div[contains(@class, "xans-product-listmore")]/a[contains(@class, "btnMore")]'
            
            try:
                next_page_button = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, more_button_xpath))
                )
                print(f"\n[{current_page_number} í˜ì´ì§€] 'ë”ë³´ê¸°' ë²„íŠ¼ í´ë¦­...")
                driver.execute_script("arguments[0].click();", next_page_button)
                time.sleep(2)
            except Exception:
                print(f"\n[{current_page_number} í˜ì´ì§€] 'ë”ë³´ê¸°' ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë˜í•‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            current_page_number += 1
            print(f"[{current_page_number-1} í˜ì´ì§€] ì²˜ë¦¬ ì™„ë£Œ. ì´ ì†Œìš” ì‹œê°„: {time.time() - page_start_time:.2f}ì´ˆ")
            
    finally:
        if driver:
            driver.quit()
        
        if all_product_data and len(all_product_data) > last_save_count:
            print("\n--- ìŠ¤í¬ë˜í•‘ ì¢…ë£Œ. ìµœì¢… ë°ì´í„° ì €ì¥ ---")
            df = pd.DataFrame(all_product_data)
            csv_path = os.path.join(CAT_OUTPUT_DIR, "raw_data.csv")
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        total_end_time = time.time()
        total_duration = total_end_time - total_start_time
        print(f"\n--- ìŠ¤í¬ë˜í•‘ ì‘ì—… ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {total_duration:.2f}ì´ˆ ---")

if __name__ == "__main__":
    main()